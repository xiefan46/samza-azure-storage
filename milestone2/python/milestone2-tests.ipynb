{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 1\n",
    "KB = 1024 * B\n",
    "MB = 1024 * KB\n",
    "GB = 1024 * MB\n",
    "TB = 1024 * GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "\n",
    "#command line tool\n",
    "def run_cmd(cmd, throw_exception = False):\n",
    "    p = Popen(cmd , shell=True, stdout=PIPE, stderr=PIPE)\n",
    "    out, err = p.communicate()\n",
    "    out_str = out.decode(\"utf-8\")\n",
    "    err_str = err.decode(\"utf-8\")\n",
    "    if(throw_exception and err_str != ''):\n",
    "        raise Exception(err_str)\n",
    "    return out_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogParser():\n",
    "    def __init__(self, log):\n",
    "        self.log = log\n",
    "        self.lines = log.split(\"\\n\")\n",
    "\n",
    "    def parse_all(self):\n",
    "        self.base_res_line = None\n",
    "        self.delay_percentiles_line = None\n",
    "        for line in self.lines:\n",
    "            self.extract_basic_res_line(line)\n",
    "            self.extract_delay_percentiles_line(line)\n",
    "\n",
    "    def extract_basic_res_line(self, line):\n",
    "        if self.base_res_line == None and \"micros/op\" in line and \"ops/sec\" in line:\n",
    "            self.base_res_line = line\n",
    "\n",
    "    def extract_delay_percentiles_line(self, line):\n",
    "        if self.delay_percentiles_line == None and line.startswith(\"Percentiles: P50\"):\n",
    "            self.delay_percentiles_line = line\n",
    "\n",
    "    def dump(self):\n",
    "        print(self.base_res_line)\n",
    "        print(self.delay_percentiles_line)\n",
    "\n",
    "    def get_basic_res(self):\n",
    "        return self.convert_to_dict([\"micros/op\", \"ops/sec\", \"MB/s\"], self.extract_float(self.base_res_line))\n",
    "\n",
    "    def get_percentiles(self):\n",
    "        return self.convert_to_dict([\"P50\", \"P75\", \"P99\", \"P99.9\", \"P99.99\"],\n",
    "                                    self.extract_float(self.delay_percentiles_line))\n",
    "\n",
    "    def extract_float(self, line):\n",
    "        strs = self.delay_percentiles_line.split(\" \")\n",
    "        res_list = []\n",
    "        for s in strs:\n",
    "            if self.is_float(s):\n",
    "                res_list.append(float(s))\n",
    "        return res_list\n",
    "\n",
    "    def convert_to_dict(self, name_list, num_list):\n",
    "        res_dict = {}\n",
    "        for i in range(len(name_list)):\n",
    "            res_dict[name_list[i]] = num_list[i]\n",
    "        return res_dict\n",
    "\n",
    "    def is_float(self, s):\n",
    "        try:\n",
    "            f = float(s)\n",
    "            return True\n",
    "        except:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'util'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6e06757d8ab9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBenchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'util'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import util\n",
    "\n",
    "class Benchmark():\n",
    "\n",
    "    def __init__(self, root_dir, db_bench, key_size, value_size, data_size, other_params=\"\",\n",
    "                 save_res=True, verbose=True):\n",
    "        self.db_bench = db_bench\n",
    "        self.key_size = key_size\n",
    "        self.value_size = value_size\n",
    "        self.data_size = data_size\n",
    "        self.other_params = other_params\n",
    "        self.save_res = save_res\n",
    "        self.verbose = verbose\n",
    "        self.test_dir, self.performance_testing_log_dir = self.setup_test_env(root_dir)\n",
    "        self.num_keys = self.get_num_keys(self.data_size, self.key_size, self.value_size)\n",
    "\n",
    "    def fillseq(self):\n",
    "        return self.run_db_bench(benchmarks=\"fillseq\", use_existing_db=False)\n",
    "\n",
    "    def readseq(self):\n",
    "        return self.run_db_bench(benchmarks=\"readseq\")\n",
    "\n",
    "    def overwrite(self):\n",
    "        return self.run_db_bench(benchmarks=\"overwrite\")\n",
    "\n",
    "    def readrandom(self):\n",
    "        return self.run_db_bench(benchmarks=\"readrandom\")\n",
    "\n",
    "    def readwhilewriting(self):\n",
    "        return self.run_db_bench(benchmarks=\"readwhilewriting\")\n",
    "\n",
    "    def deleterandom(self):\n",
    "        return self.run_db_bench(benchmarks=\"deleterandom\")\n",
    "\n",
    "    def benchmark_all(self):\n",
    "        self.fillseq()\n",
    "        self.readseq()\n",
    "        self.overwrite()\n",
    "        self.readrandom()\n",
    "        self.readwhilewriting()\n",
    "\n",
    "    def get_num_keys(self, data_size, key_size, value_size):\n",
    "        return int(data_size / (key_size + value_size))\n",
    "\n",
    "    def run_db_bench(self, benchmarks, use_existing_db=True, num_keys = -1):\n",
    "        if num_keys <= 0:\n",
    "            num_keys = self.get_num_keys(self.data_size, self.key_size, self.value_size)\n",
    "        const_params = \"\\\n",
    "          --db={} \\\n",
    "          --histogram=1 \\\n",
    "          --num={} \\\n",
    "          --use_existing_db={} \\\n",
    "          --key_size={} \\\n",
    "          --value_size={}  \\\n",
    "          --block_size=4096 \\\n",
    "          --compression_type=snappy \\\n",
    "          --max_write_buffer_number=3 \\\n",
    "          --write_buffer_size=33554432 \\\n",
    "          --cache_size=104857600 \\\n",
    "          --statistics {}\".format(self.test_dir, num_keys, 1 if use_existing_db else 0, self.key_size, self.value_size,\n",
    "                                  self.other_params)\n",
    "        command = \"{} --benchmarks=\\\"{}\\\"  {} \".format(self.db_bench, benchmarks, const_params)\n",
    "        if self.verbose:\n",
    "            print(\"command : {}\".format(command))\n",
    "        res = run_cmd(command, False)\n",
    "        if self.save_res == True:\n",
    "            self.dump_res_to_file(command, res, benchmarks)\n",
    "        if self.verbose == True:\n",
    "            print(res)\n",
    "        return res\n",
    "\n",
    "    def dump_res_to_file(self, command, res, benchmarks):\n",
    "        file_name = \"{}/{}-performance-result\".format(self.performance_testing_log_dir, benchmarks)\n",
    "        if self.verbose:\n",
    "            print(\"Dump result to file : {}\".format(file_name))\n",
    "        file = open(file_name, \"w+\")\n",
    "        file.write(\"{}\\n\\n\".format(command))\n",
    "        file.write(res)\n",
    "        file.close()\n",
    "\n",
    "    def setup_test_env(self, root_dir):\n",
    "        test_dir = self.get_random_test_dir(root_dir)\n",
    "        performance_testing_log_dir = \"{}/performance-testing-log\".format(test_dir)\n",
    "        #if self.verbose:\n",
    "        print(\"Setup a new test root. Test root dir {}\".format(test_dir))\n",
    "        self.create_new_dir(test_dir)\n",
    "        self.create_new_dir(performance_testing_log_dir)\n",
    "        return test_dir, performance_testing_log_dir\n",
    "\n",
    "    def create_new_dir(self, dir_name):\n",
    "        os.system(\"rm -rf {}\".format(dir_name))\n",
    "        os.system(\"mkdir {}\".format(dir_name))\n",
    "\n",
    "    def get_random_test_dir(self, root_dir):\n",
    "        return \"{}/test-{}\".format(root_dir, random.randint(0, 1000000000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Benchmark1 : Get/Put latencies for different key/values and different mode (random/sequential/readwhilewrite)\n",
    "\n",
    "key/value : [10B, 100B], [100B, 1KB], [1KB, 1MB]\n",
    "'''\n",
    "\n",
    "def print_log(log):\n",
    "    parser = LogParser(log)\n",
    "    parser.parse_all()\n",
    "    parser.dump()\n",
    "    \n",
    "def benchmark1(root, db_bench, data_size):\n",
    "    \n",
    "    print(\"Total Data Size : {}\".format(data_size))\n",
    "    \n",
    "    total_start = time.time()\n",
    "    for kv in [[10*B, 100*B], [100*B, 1*KB], [1*KB, 1*MB]]:\n",
    "        \n",
    "        benchmark = Benchmark(root_dir = root, db_bench = db_bench, key_size = kv[0], value_size = kv[1],\n",
    "                       data_size = data_size, other_params = \"\", save_res=True, verbose=False)\n",
    "        \n",
    "        print(\"Key : {} Value : {} \\n\".format(kv[0], kv[1]))\n",
    "        methods = [benchmark.fillseq, benchmark.readseq, benchmark.overwrite, \n",
    "                   benchmark.readrandom, benchmark.deleterandom]\n",
    "        for method in methods:\n",
    "            start = time.time()\n",
    "            log = method()\n",
    "            print_log(log)\n",
    "            print(\"time : {}\".format(time.time() - start))\n",
    "            print(\"\\n\")\n",
    "        print(\"\\n\\n\\n\\n\")\n",
    "    print(\"total time cost : {} seconds\".format(time.time() - total_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_in_laptop():\n",
    "    ROOT = \"/Users/fxie/Desktop/intern_project/samza-azure-storage/milestone2/db_data\"\n",
    "    DB_BENCH = \"~/Desktop/intern_project/rocksdb/db_bench\"\n",
    "    DATA_SIZE = 1 * GB\n",
    "    benchmark1(ROOT, DB_BENCH, DATA_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_in_laptop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
